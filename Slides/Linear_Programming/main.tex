\documentclass{beamer}

\input{settings.tex}


\title{Linear Programming}
\subtitle{Computational Intelligence, Lecture 8}
\author{by Sergei Savin}
\centering
\date{\mydate}

\begin{document}
\maketitle


\begin{frame}{Content}

\begin{itemize}
\item Linear Programming
\item Convex piece-wise linear functions
\item Chebyshev center of a polyhedron
\item Linear-Fractional Programming
\item Homework
\end{itemize}

\end{frame}



\begin{frame}{Linear Programming}
\framesubtitle{General form}
\begin{flushleft}

A linear program (LP) is an optimization problem of the form:

\begin{equation} \label{LP}
\begin{aligned}
& \underset{\mathbf{x}}{\text{minimize}}
& & \mathbf{f}^\top \mathbf{x} , \\
& \text{subject to}
& & \begin{cases} 
\mathbf{A}\mathbf{x} \leq \mathbf{b}, \\
\mathbf{C}\mathbf{x} = \mathbf{d}.
\end{cases}
%
\end{aligned}
\end{equation}

It is one of the older and widely used classes of convex optimization problems. 

\bigskip

Note that the solution of such problem will always lie on the boundary of its domain.
 
\end{flushleft}
\end{frame}




\begin{frame}{Linear Programming}
\framesubtitle{LP with no solution - examples}
\begin{flushleft}

Here are some examples of LP which have no solutions:

\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}}{\text{minimize}}
& & \begin{bmatrix} 1 & 1 \end{bmatrix} 
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}
\end{aligned}
\end{equation}

This one is has no boundaries at all, hence no solution. Next one has boundaries, but they do not restrict motion along the descent direction for the cost function.

\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}}{\text{minimize}}
& & \begin{bmatrix} 1 & 1 \end{bmatrix} 
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} , \\
& \text{subject to}
& & \begin{bmatrix} 1 & 0 \end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \leq
1
%
\end{aligned}
\end{equation}

 
\end{flushleft}
\end{frame}



\begin{frame}{Convex piece-wise linear functions}
\framesubtitle{Problem statement}
\begin{flushleft}

Convex piece-wise linear functions have the form:

\begin{equation}
    f(\mathbf{x}) = \text{max}(\mathbf{a}_i^\top \mathbf{x} + b_i)
\end{equation}

Figure below shows geometric interpretation of such function for a one-dimensional case.

\begin{figure} [h!]
\begin{center}
\input{fig_1.tex}
\end{center} 
% \caption{Visualization of trajectory generation done in the developed software}
\end{figure}

 
\end{flushleft}
\end{frame}




\begin{frame}{Convex piece-wise linear functions}
\framesubtitle{Solution as LP}
\begin{flushleft}

We can formulate a minimization problem using convex piece-wise linear functions:

\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}}{\text{minimize}}
& & \text{max}(\mathbf{a}_i^\top \mathbf{x} + b_i)
\end{aligned}
\end{equation}

\bigskip

Which can be equivalently transformed into the following LP:

\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}, t}{\text{minimize}}
& & t \\
& \text{subject to}
& & \mathbf{a}_i^\top \mathbf{x} + b_i \leq t
%
\end{aligned}
\end{equation}

We can observe that optimal (minimal) $t$ will have to lie on one of the linear functions $\mathbf{a}_i^\top \mathbf{x} + b_i$, i.e. on the original piece-wise linear function $f(\mathbf{x})$. And optimal value on t corresponds to the smallest value of the original function $f(\mathbf{x})$.
 
\end{flushleft}
\end{frame}



\begin{frame}{Sum of piece-wise linear functions}
\framesubtitle{Solution as LP}
\begin{flushleft}


Sum of convex piece-wise linear functions have the form:

\begin{equation}
    f(\mathbf{x}) + g(\mathbf{x}) = \text{max}(\mathbf{a}_i^\top \mathbf{x} + b_i) +  \text{max}(\mathbf{c}_i^\top \mathbf{x} + d_i)
\end{equation}

\bigskip

Their representation as LP is:

\begin{equation}
\begin{aligned}
& \underset{\mathbf{x}, t_1, t_2}{\text{minimize}}
& & t_1 + t_2 \\
& \text{subject to}
& & \begin{cases}
\mathbf{a}_i^\top \mathbf{x} + b_i \leq t_1 \\
\mathbf{c}_i^\top \mathbf{x} + d_i \leq t_2
\end{cases}
%
\end{aligned}
\end{equation}


 
\end{flushleft}
\end{frame}




\begin{frame}{Convex piece-wise linear functions}
\framesubtitle{Code}
\begin{flushleft}

\input{code1}
 
\end{flushleft}
\end{frame}



\begin{frame}{Chebyshev center of a polyhedron}
\framesubtitle{Problem statement}
\begin{flushleft}

Chebyshev center of a polyhedron is the center of the largest ball inscribed in a polyhedron:

\begin{figure} [h!]
\begin{center}
\input{fig_2.tex}
\end{center} 
% \caption{Visualization of trajectory generation done in the developed software}
\end{figure}

Equation describing this ball can be written as:

\begin{equation}
    \mathcal{B} = \{ \mathbf{x}_c + \mathbf{u}: \ ||\mathbf{u}||_2 \leq r \}
\end{equation}

where $r$ is the radius of the ball and $\mathbf{x}_c$ is its center.
 
\end{flushleft}
\end{frame}



\begin{frame}{Chebyshev center of a polyhedron}
	\framesubtitle{Max over the dot product}
	\begin{flushleft}
		
		Before we move towards solving the problem, let us consider the following maximization: 
		
		\begin{equation}
			\text{sup} \{ \mathbf{a}^\top \mathbf{u}: \ ||\mathbf{u}||_2 \leq r \}
		\end{equation}
	
	We can re-write the expression:
	
		\begin{equation}
			\text{sup} \{ \mathbf{a}^\top \mathbf{u}: \ ||\mathbf{u}||_2 \leq r \}  = 
			\text{sup} \{ ||\mathbf{a}|| \cdot ||\mathbf{u}|| \text{cos}(\varphi): \ ||\mathbf{u}||_2 \leq r \}
		\end{equation}
%	
where $\varphi$ is the angle between $\mathbf{a}$ and $\mathbf{u}$. Since $\mathbf{a}$ is constant, $\text{max}(||\mathbf{u}||) = r$, and $\text{max}(\text{cos}(\varphi)) = 1$, we get:
	
	\begin{equation}
		\text{sup} \{ \mathbf{a}^\top \mathbf{u}: \ ||\mathbf{u}||_2 \leq r \}  = 
		 ||\mathbf{a}|| r
	\end{equation}
	
		
	\end{flushleft}
\end{frame}




\begin{frame}{Chebyshev center of a polyhedron}
\framesubtitle{Solution as LP, part one}
\begin{flushleft}

For the ball $\mathcal{B}$ to be inscribed in a polygon $\mathcal{P} = \{ \mathbf{x}: \ \mathbf{A}\mathbf{x} \leq \mathbf{b} \}$, the following should hold:

\begin{equation}
    \text{sup} \{ \mathbf{a}_i^\top (\mathbf{x}_c + \mathbf{u}): \ ||\mathbf{u}||_2 \leq r \} \leq b_i
\end{equation}

Note that the largest value of $\mathbf{a}_i^\top \mathbf{u}$ under condition $||\mathbf{u}||_2 \leq r$ is $r ||\mathbf{a}_i||$: it can indeed achieve this value if $\mathbf{a}_i$ and $\mathbf{u}$ are co-directional, but a larger one is not possible. Therefore:

\begin{equation}
    \text{sup} \{ \mathbf{a}_i^\top (\mathbf{x}_c + \mathbf{u}): \ ||\mathbf{u}||_2 \leq r \}  = 
    \mathbf{a}_i^\top \mathbf{x}_c + r ||\mathbf{a}_i||
    \leq b_i
\end{equation}

 
\end{flushleft}
\end{frame}



\begin{frame}{Chebyshev center of a polyhedron}
\framesubtitle{Solution as LP, part two}
\begin{flushleft}

Finally, we can write down the solution of the problem as a linear optimization:

\begin{equation}
\begin{aligned}
& \underset{r, \ \mathbf{x}_c}{\text{maximize}}
& & r \\
& \text{subject to}
& & \mathbf{a}_i^\top \mathbf{x}_c + r ||\mathbf{a}_i||
    \leq b_i
%
\end{aligned}
\end{equation}

 
\end{flushleft}
\end{frame}




\begin{frame}{Chebyshev center of a polyhedron}
\framesubtitle{Code}
\begin{flushleft}

Below we can see MATLAB code for solving the problem:

\input{code2.tex}

 
\end{flushleft}
\end{frame}




\begin{frame}{Linear-Fractional Programming}
	\framesubtitle{Formulation}
	\begin{flushleft}
		
		The following is the Linear-Fractional Programming problem:
		
		\begin{equation}
			\begin{aligned}
				& \underset{\mathbf{x}}{\text{maximize}}
				& & \frac{\mathbf{c}^\top \mathbf{x} + d}{\mathbf{e}^\top \mathbf{x} + f} \\
				& \text{subject to}
				& & 
				 \begin{cases}
				 	\mathbf{A} \mathbf{x} \leq \mathbf{b} \\
				 	\mathbf{A}_e \mathbf{x} = \mathbf{b}_e
				 \end{cases}
			\end{aligned}
		\end{equation}
		
		This doesn't look like an LP, but let us see if we can try to bring this problem into this form. 
		
	\end{flushleft}
\end{frame}



\begin{frame}{Linear-Fractional Programming}
	\framesubtitle{LP form}
	\begin{flushleft}
		
		The following is the Linear-Fractional Programming problem in LP form:
		
		\begin{equation}
			\begin{aligned}
				& \underset{\mathbf{y}, \ z}{\text{maximize}}
				& & \mathbf{c}^\top \mathbf{y} + z d \\
				& \text{subject to}
				& & 
				\begin{cases}
					\mathbf{A} \mathbf{y} \leq z \mathbf{b} \\
					\mathbf{A}_e \mathbf{y} = z \mathbf{b}_e \\
					\mathbf{e}^\top \mathbf{y} + z f = 1 \\
					z \geq 0
				\end{cases}
			\end{aligned}
		\end{equation}
		
		Here the variables $\mathbf{y}$ and $z$ are related to $\mathbf{x}$ as follows.
		
		\begin{equation}
			\mathbf{y} = \frac{\mathbf{x}}{\mathbf{e}^\top \mathbf{x} + f}
		\end{equation}
	 
	\begin{equation}
	\mathbf{z} = \frac{1}{\mathbf{e}^\top \mathbf{x} + f}
	\end{equation} 
		
	\end{flushleft}
\end{frame}



\begin{frame}{Linear-Fractional Programming}
	\framesubtitle{details}
	\begin{flushleft}
		
		We assumed that the domain of the previous problem is limited to $\mathbf{e}^\top \mathbf{x} + f \geq 0$. With that we have:
		
		\begin{equation}
			 \mathbf{c}^\top \mathbf{y} + z d 
			 = 
			 \mathbf{c}^\top\frac{\mathbf{x}}{\mathbf{e}^\top \mathbf{x} + f} + \frac{1}{\mathbf{e}^\top \mathbf{x} + f} d 
			 =
			 \frac{\mathbf{c}^\top\mathbf{x} + d}{\mathbf{e}^\top \mathbf{x} + f}
		\end{equation}
		
		
		\begin{equation}
			\mathbf{A} \mathbf{y} \leq z \mathbf{b} 
			\implies
			\mathbf{A} \frac{\mathbf{x}}{\mathbf{e}^\top \mathbf{x} + f} \leq \frac{1}{\mathbf{e}^\top \mathbf{x} + f} \mathbf{b} 
			\implies
			\mathbf{A} \mathbf{x} \leq \mathbf{b} 
		\end{equation}
		
		
	\end{flushleft}
\end{frame}





\begin{frame}{Homework}
% \framesubtitle{Parameter estimation}
\begin{flushleft}

Implement linear approximation of a convex function and solve it as LP

\end{flushleft}
\end{frame}



\begin{frame}
	\centerline{Lecture slides are available via Moodle.}
	\bigskip
	\centerline{You can help improve these slides at:}
	\centerline{
		\mygit
	}
	\bigskip
	
	\textcolor{black}{\qrcode[height=1.5in]{https://github.com/SergeiSa/Computational-Intelligence-Slides-Spring-2022}}
	\bigskip
	
	
	\centerline{Check Moodle for additional links, videos, textbook suggestions.}
\end{frame}


\end{document}
