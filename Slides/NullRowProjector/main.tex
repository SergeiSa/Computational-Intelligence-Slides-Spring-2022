\documentclass{beamer}

\input{settings.tex}


\title{Least Squares, Null space, Row space, Projectors}
\subtitle{Computational Intelligence, Lecture 2}
\author{by Sergei Savin}
\centering
\date{\mydate}



\begin{document}
\maketitle


\begin{frame}{Content}

\begin{itemize}
\item Motivating questions
\item Four Fundamental Subspaces
\item Null space
\begin{itemize}
    \item Definition
    \item Calculation
\end{itemize}
\item Null space projection
\item Closest element from a linear subspace
\item Orthogonality, definition
\item Projection
\item Vectors in Null space, Row space
\item Row and Null spaces in linear equations
% \item Read more
\end{itemize}

\end{frame}




\begin{frame}{Least squares at a glance (1)}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
	Consider the following problem: find the smallest 2-norm $\bo{x}$ for which the equality $\bo{A}\bo{x} = \bo{y}$ has least residual. This is the \emph{least squares problem}.
	
\begin{itemize}
	\item Residual is defined as $\bo{e} = \bo{A}\bo{x} - \bo{y}$
	\item The idea that we need to find the smallest 2-norm $\bo{x}$ and, at the same time, least residual $\bo{e}$ seems to call for compromise. However, we will find that there exist a linear subspace of solutions with the exact same residual, and on that subspace we can find $\bo{x}$ with the smallest norm.
\end{itemize}	
		
	\end{flushleft}
\end{frame}




\begin{frame}{Least squares at a glance (2)}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
		Minimum of $||\bo{e}||_2 = ||\bo{A}\bo{x} - \bo{y}||_2$ is achieved at the same $\bo{x}$ as minimum of $(\bo{A}\bo{x} - \bo{y})^\top(\bo{A}\bo{x} - \bo{y})$. With that in mind, let us find its extremum:
		
		\begin{equation}
			2\bo{A}^\top(\bo{A}\bo{x} - \bo{y}) = 0
		\end{equation}
		
		\begin{equation}
			\bo{A}^\top\bo{A}\bo{x} = \bo{A}^\top\bo{y}
		\end{equation}
	
		\begin{equation}
			\bo{x} = (\bo{A}^\top\bo{A})^{-1} \bo{A}^\top\bo{y}
		\end{equation}
		
		Thus we can define a \emph{pseudoinverse}:
		
		\begin{equation}
			\bo{A}^+ = (\bo{A}^\top\bo{A})^{-1} \bo{A}^\top
		\end{equation}
		
		
	\end{flushleft}
\end{frame}



\begin{frame}{Least squares at a glance (3)}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
		Thus the least residual solution to $\bo{A}\bo{x} = \bo{y}$ is written as:
		
		\begin{equation}
			\bo{x} = \bo{A}^+\bo{y}
		\end{equation}
		
		We proved that this is the least-residual solution, we will prove that the solution is smallest norm (out of all solutions with the same residual) in the following lectures.
		
	\end{flushleft}
\end{frame}



\begin{frame}{Least squares and closest element}
	% \framesubtitle{Parameter estimation}
	\begin{flushleft}
		
		You are given an equation $\bo{A}\bo{x} = \bo{y}$. You want to find an $\bo{x}$ that gets the value of $\bo{y}$ as close as possible to $\bo{y}^*$ in terms of the 2-norm.
		
		\bigskip
		
		We know that $\bo{x} = \bo{A}^+\bo{y}^*$ gives us the least residual solution. Multiplying it by $\bo{A}$ we get:
		
		\begin{equation}
			\bo{y} = \bo{A}\bo{A}^+\bo{y}^*
		\end{equation}
	
	This is the value of $\bo{y}$ that we can achieve, which is closest to $\bo{y}^*$.
	
	\end{flushleft}
\end{frame}



%\begin{frame}{Motivating questions}
%% \framesubtitle{Parameter estimation}
%\begin{flushleft}
%
%You have a linear operator $\mathbf A$. Try to answer the following questions:
%
%\begin{itemize}
%    \item What are all vectors this operator can produce as outputs? How to find them?
%    \item Are there two inputs that make it produce the same output?
%    \item Are there inputs that produce zero as an output?
%    \item Are there outputs that cannot be produced? 
%    \item What is the smallest vector $\bo{x}$ that produces given output $\bo{y}$? 
%\end{itemize}
%
%These questions are directly related to the idea of fundamental subspaces of a linear operator.
%
%\end{flushleft}
%\end{frame}


\begin{frame}{Four Fundamental Subspaces}
% \framesubtitle{Parameter estimation}
\begin{flushleft}

One of the key ideas in Linear Algebra is that every linear operator has four fundamental subspaces:

\begin{itemize}
    \item Null space
    \item Row space
    \item Column space
    \item Left null space
\end{itemize}

\bigskip

Our goal is to understand them. The usefulness of this concept is enormous.

\end{flushleft}
\end{frame}

\begin{frame}{Null space}
\framesubtitle{Definition}
\begin{flushleft}

Consider the following task: find all solutions to the system of equations $\mathbf{A} \mathbf{x} = \mathbf{0}$.

\bigskip

It can be re-formulated as follows: find all elements of the \emph{null space} of $\mathbf{A}$.

\begin{block}{Definition 1}
  \emph{Null space} of $\mathbf{A}$ is the set of all vectors $\mathbf{x}$ that $\mathbf{A}$ maps to $\mathbf{0}$
\end{block}

\bigskip

We will denote null space as $\mathcal{N}(\mathbf{A})$. In the literature, it is often denoted as $\text{ker}(\mathbf{A})$ or $\text{null}(\mathbf{A})$.

\end{flushleft}
\end{frame}


\begin{frame}{Null space}
\framesubtitle{Calculation}
\begin{flushleft}

Now we can find all solutions to the system of equations $\mathbf{A} \mathbf{x} = \mathbf{0}$ by using functions that generate an orthonormal \emph{basis} in the null space of $\mathbf{A}$. In MATLAB it is function \texttt{null}, in Python/Scipy - \texttt{null\_space}:

\bigskip

\begin{itemize}
    \item \texttt{N = null(A)}.
    \item \texttt{N = scipy.linalg.null\_space(A)}.
\end{itemize}

\bigskip

That is it! Space of solutions of $\mathbf{A} \mathbf{x} = \mathbf{0}$ is the span of the columns of $\mathbf{N}$, and all solutions $\mathbf{x}^*$ can be represented as $\mathbf{x}^* = \mathbf{N}\mathbf{z}$; for any $\mathbf{z}$ we get a unique solution, and for any solution - a unique $\mathbf{z}$.

\end{flushleft}
\end{frame}



\begin{frame}{Null space projection}
\framesubtitle{Local coordinates}
\begin{flushleft}

Let $\bo{N}$ be the orthonormal basis in the null space of matrix $\bo{A}$. Then, if a vector $\bo{x}$ lies in the null space of $\bo{A}$, it can be represented as:

\begin{equation}
    \bo{x} = \bo{N}\bo{z}
\end{equation}
%
where $\bo{z}$ are coordinates of $\bo{x}$ in the basis $\bo{N}$.

\bigskip

However, there are vectors which not only are not lying in the null space of $\bo{A}$,  but the closest vector to them in the null space is the zero vector.

\end{flushleft}
\end{frame}


\begin{frame}{Closest element from a linear subspace}
% \framesubtitle{Orthogonality, examples}
\begin{flushleft}

Let $\bo{A} = \begin{bmatrix} 0 & 1 \\ 0 & 0\end{bmatrix}$. Its null space has orthonormal basis $\bo{N} = \begin{bmatrix} 1 \\ 0\end{bmatrix}$. 

\begin{itemize}
    \item $\begin{bmatrix} -2 \\ 0 \end{bmatrix} = 
    -2 \bo{N}$,
    $\begin{bmatrix} 10 \\ 0 \end{bmatrix} = 
    10 \bo{N}$, - both are in the null space.
    \item for $\bo{x} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ the closest vector in the null space is $\begin{bmatrix} 1 \\ 0 \end{bmatrix}$.
    \item for $\bo{y} = \begin{bmatrix} 0 \\ 2 \end{bmatrix}$ the closest vector in the null space is $\begin{bmatrix} 0 \\ 0 \end{bmatrix}$.
\end{itemize}


\end{flushleft}
\end{frame}



\begin{frame}{Orthogonality, definition (1)}
% \framesubtitle{Orthogonality, definition}
\begin{flushleft}

\begin{definition}
Any two vectors, $\bo{x}$ and $\bo{y}$, whose dot product is zero are said to be \emph{orthogonal} to each other.
\end{definition}

\begin{definition}
	Vector $\bo{x}$, whose dot product with any $\bo{y} \in \mathcal{L}$ is orthogonal to the subspace $\mathcal{L}$
\end{definition}

\begin{definition}[equivalent]
	 If for a vector $\bo{x}$, the closest vector to it from a linear subspace $\mathcal{L}$ is zero vector, $\bo{x}$ is called orthogonal to the subspace $\mathcal{L}$.
\end{definition}


\end{flushleft}
\end{frame}


\begin{frame}{Orthogonality, definition (2)}
	% \framesubtitle{Orthogonality, definition}
	\begin{flushleft}
		
		\begin{definition}
			The space of all vectors $\bo{y}$, orthogonal to a linear subspace $\mathcal{L}$ is called \emph{orthogonal complement} of $\mathcal{L}$ and is denoted as $\mathcal{L}^\perp$.
		\end{definition}
		
		
		\begin{definition}[equivalent]
			The space of all vectors $\bo{y}$, such that $\text{dot}(\bo{y}, \bo{x}) = 0$, $\forall \bo{x} \in \mathcal{L}$ is called \emph{orthogonal complement} of $\mathcal{L}$.
		\end{definition}
		
		Therefore $\bo{x} \in \mathcal{L}$ and $\bo{y} \in \mathcal{L}^\perp$ implies $\text{dot}(\bo{y}, \bo{x}) = 0$.
		
	\end{flushleft}
\end{frame}





\begin{frame}{Projection}
\framesubtitle{Part 1}
\begin{flushleft}

Let $\bo{L}$ be an orthonormal basis in a linear subspace $\mathcal{L}$. Take vector $\bo{a} = \bo{x} + \bo{y}$, where $\bo{x}$ lies in the subspace $\mathcal{L}$, and $\bo{y}$ lies in the subspace $\mathcal{L}^\perp$.

\bigskip

\begin{definition}
We call such vector $\bo{x}$ a \emph{projection} of $\bo{a}$ onto subspace $\mathcal{L}$, and such vector $\bo{y}$ a projection of $\bo{a}$ onto subspace $\mathcal{L}^\perp$
\end{definition}

\bigskip

Projection is the closest element in the subspace to a given vector. Projection of $\bo{a}$ onto $\mathcal{L}$ can be found as: 

\begin{equation}
    \bo{x} = \bo{L} \bo{L}^+ \bo{a}
\end{equation}

Since $\bo{L}$ is orthonormal, this is the same as $\bo{x} = \bo{L} \bo{L}^\top \bo{a}$

\end{flushleft}
\end{frame}



\begin{frame}{Projection}
\framesubtitle{Part 2}
\begin{flushleft}

Since $\bo{a} = \bo{x} + \bo{y}$, and $\bo{x} = \bo{L} \bo{L}^\top \bo{a}$, we can write:

\begin{equation}
    \bo{a} = \bo{L} \bo{L}^\top \bo{a} + \bo{y}
\end{equation}
%
from which it follows that the projection of $\bo{a}$ onto $\mathcal{L}^\perp$ can be found as: 

\begin{equation}
    \bo{y} = (\bo{I} - \bo{L} \bo{L}^\top) \bo{a}
\end{equation}
%
where $\bo{I}$ is an identity matrix. Since $\bo{L}$ is orthonormal, this is the same as $\bo{y} = (\bo{I} - \bo{L} \bo{L}^\top) \bo{a}$

\end{flushleft}
\end{frame}




\begin{frame}{Row space}
\framesubtitle{Definition}
\begin{flushleft}

\begin{definition}
Let $\mathcal{N}$ be null space of $\bo{A}$. Then orthogonal subspace $\mathcal{N}^\perp$ is called \emph{row space} of $\bo{A}$.
\end{definition}

\begin{definition}
\emph{Row space} of $\bo{A}$ is the space of all smallest-norm solutions of $\bo{A}\bo{x} = \bo{y}$, for $\forall \bo{y}$.
\end{definition}

\bigskip

We will denote row space as $\mathcal{R}$.

\end{flushleft}
\end{frame}




\begin{frame}{Vectors in Null space, Row space}
% \framesubtitle{Definition}
\begin{flushleft}

Given vector $\bo{x}$, matrix $\bo{A}$ and its nulls space basis $\bo{N}$, and we check if $\bo{x}$ is in the null space of $\bo{A}$. The simplest way is to check if $\bo{A}\bo{x} = 0$. But sometimes we may want to avoid computing $\bo{A}\bo{x}$, for example if the number of elements of $\bo{A}$ is much larger than the number of elements of $\bo{N}$.

\bigskip

If $\bo{x}$ is in the null space of $\bo{A}$, it will have zero projection onto the row space of $\bo{A}$. This gives us the condition we can check:

\begin{equation}
    (\bo{I} - \bo{N} \bo{N}^\top) \bo{x} = 0
\end{equation}

By the same logic, condition for being in the row space is as follows:

\begin{equation}
    \bo{N} \bo{N}^\top \bo{x} = 0
\end{equation}


\end{flushleft}
\end{frame}







\begin{frame}{Row and Null spaces in linear equations}
\framesubtitle{Part 1}
\begin{flushleft}

Consider another task: find all solutions to the system of equations $\bo{A} \bo{x} = \bo{y}$.

\bigskip

Assume we have two solutions to the system: $\bo{x}_1$ and $\bo{x}_2$. We know that $\bo{A} \bo{x}_1 = \bo{A} \bo{x}_2= \bo{y}$, hence $\bo{A} (\bo{x}_1 - \bo{x}_2) = \bo{0}$. In other words, the difference between any two solutions lies in the null space of $\bo{A}$.

\bigskip

On the other hand, let $\bo{x}^*$ be a solution, and $\bo{x}^N \in \mathcal{N}(\bo{A})$ be a vector in the null space of $\bo{A}$. Then $\bo{x}^* + \bo{x}^N$ is also a solution, since $\bo{A} (\bo{x}^* + \bo{x}^N) = \bo{A} \bo{x}^* + \bo{A} \bo{x}^N = \bo{A} \bo{x}^* = \bo{y}$.

\bigskip

Therefore, the solution space is given by a single partial solution $\bo{x}^p \notin \mathcal{N}(\bo{A})$ and the whole null space of $\bo{A}$.

\end{flushleft}
\end{frame}


\begin{frame}{Row and Null spaces in linear equations}
\framesubtitle{Part 2}
\begin{flushleft}

There are infinitely many ways to chose $\bo{x}^p$, since if $\bo{x}^p \notin \mathcal{N}(\bo{A})$, then $(\bo{x}^p + \bo{x}^N) \notin \mathcal{N}(\bo{A})$, if $\bo{x}^N \in \mathcal{N}(\bo{A})$. However: 

\begin{block}{Statement 1}
The smallest-norm $\bo{x}^p$ will lie in the row space of $\bo{A}$.
\end{block}

\bigskip

We can prove it by observing that there can be only one $\bo{x}^p \in \mathcal{R}(\bo{A})$ and adding to it any vector $\bo{x}^N \in \mathcal{N}(\bo{A})$ can only increase its magnitude, as $\bo{x}^p$ and $\bo{x}^N$ are orthogonal.

\end{flushleft}
\end{frame}



\begin{frame}{Row and Null spaces in linear equations}
\framesubtitle{Part 3}
\begin{flushleft}

If we have $\bo{x}^*$, which is a solution to $\bo{A} \bo{x} = \bo{y}$, we can find the particular solution $\bo{x}^p \in \mathcal{R}(\bo{A})$ as a projection:

\begin{equation}
    \bo{x}^p = (\bo{I} - \bo{N} \bo{N}^\top) \bo{x}^*
\end{equation}
%
where $\bo{N}$ is the null space basis for $\bo{A}$. Alternatively, we can simply find it as:

\begin{equation}
    \bo{x}^p = \bo{A}^+ \bo{y}
\end{equation}

\bigskip

All solutions to $\bo{A} \bo{x} = \bo{y}$ are then given as:

\begin{equation}
    \bo{x}^* = \bo{A}^+ \bo{y} + \bo{N}\bo{z}, \ \forall \bo{z}
\end{equation}

\end{flushleft}
\end{frame}




\begin{frame}
	\centerline{Lecture slides are available via Moodle.}
	\bigskip
	\centerline{You can help improve these slides at:}
	\centerline{
		\mygit
	}
	\bigskip
	
	\textcolor{black}{\qrcode[height=1.5in]{https://github.com/SergeiSa/Computational-Intelligence-Slides-Spring-2022}}
	\bigskip
	
	
	\centerline{Check Moodle for additional links, videos, textbook suggestions.}
\end{frame}

\end{document}
